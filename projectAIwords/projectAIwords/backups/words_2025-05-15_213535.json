{
    "β‐VAE": {
        "meaning": "変分オートエンコーダに正則化項を追加したモデル",
        "memo": "潜在空間の独立性を強化し、解釈可能な特徴抽出を目指す。"
    },
    "infoVAE": {
        "meaning": "情報理論を導入した変分オートエンコーダ",
        "memo": "潜在変数と生成データの相互情報量を最大化することで、より意味のある生成を行う。"
    },
    "VQ-VAE": {
        "meaning": "ベクトル量子化を利用しタオートエンコーダ",
        "memo": "潜在空間を離散化することで、生成モデルの学習を改善。音声や画像生成に利用される。"
    },
    "潜在変数": {
        "meaning": "観測できないがモデル内部で表現される隠れた変数",
        "memo": "オートエンコーダやVAEの内部で、データの圧縮表現として扱われる。"
    },
    "統計分布": {
        "meaning": "データがどのように分布しているかを示す関数",
        "memo": "正規分布、ポアソン分布、ベルヌーイ分布など、データの特徴を把握するために使用。"
    },
    "事前学習": {
        "meaning": "大規模データであらかじめ学習しておく手法",
        "memo": "後のタスクに使うモデルの初期値として利用し、学習時間の短縮と精度向上を実現する。"
    },
    "デコード": {
        "meaning": "圧縮された潜在空間から元のデータを再構成する処理",
        "memo": "潜在変数をもとに、元データ形式に近い形で出力を生成する。"
    },
    "エンコード": {
        "meaning": "データを低次元の潜在空間に圧縮する処理",
        "memo": "入力コードの特徴を抽出し、圧縮された表現に変換する。"
    },
    "オートエンコーダ": {
        "meaning": "データを圧縮・再構成するためのニューラルネットワーク",
        "memo": "入力データを低次元の潜在空間に圧縮(エンコード)し、そこから元のデータを再構成(デコード)する。"
    },
    "可視層": {
        "meaning": "データの入力や出力を受け持つ層",
        "memo": "データをネットワーク2入力したり、出力として取得する部分。オートエンコーダではエンコーダとデコーダの両方に存在する。"
    },
    "Multi-Head Attention ": {
        "meaning": "複数のSelf-Attentionを並列に実行する仕組み",
        "memo": "多様な視点から系列データを解析することで、文脈理解の精度を高める。"
    },
    "バリュー": {
        "meaning": "Self-Attentionにおいて実際の出力に使用されるベクトル",
        "memo": "関連性に基づいて加重平均され、最終的な出力の計算に使われる。"
    },
    "キー": {
        "meaning": "Self-Attentionにおいて関連性を判断するためのベクトル",
        "memo": "各単語が持つ情報の「キー」を示し、他の単語との関連性を評価する。"
    },
    "クエリ": {
        "meaning": "Self-Attentionにおいて関連性を求めるためのベクトル",
        "memo": "対象の単語が他の単語にどれだけ依存するかを計算する。\n"
    },
    "位置エンコーディング": {
        "meaning": "入力系列の順序情報を付与する処理",
        "memo": "Self-Attentionは系列の順番を持たないため、位置情報を追加して順序を認識させる。"
    },
    "Self-Attention": {
        "meaning": "系列内の要素同士の関連性を学習する注意機構",
        "memo": "各単語が系列内の他の単語にどれだけ関連するかを計算し、文脈理解を深める。"
    },
    "Encoder-Decoder Attention": {
        "meaning": "エンコーダの出力を基にデコーダが重み付けを行う仕組み",
        "memo": "入力系列の重要部分に焦点を当てて、デコーダの出力を生成する。"
    },
    "Source-Target Attention": {
        "meaning": "エンコーダとデコーダ間の注意機構",
        "memo": "エンコーダーの出力をもとにデコーダが重要な情報に集中する仕組み。翻訳や要約などで使われる。"
    },
    "Attention": {
        "meaning": "入力系列の重要な部分に重みをつけて学習する手法",
        "memo": "翻訳や要約で効果を発揮し、重要な単語や特徴に重点を置いて処理を行う。"
    },
    "教師強制": {
        "meaning": "sequence-to-sequenceモデルの学習時に正解データを使用する手法",
        "memo": "学習中に実際の正解データを強制的に次のステップに渡すことで安定化を図る。"
    },
    "Image  Captioning": {
        "meaning": "画像の内容を文章として説明する技術",
        "memo": "CNNで画像特徴を抽出し、RNNでキャプションを生成する。"
    },
    "デコーダ": {
        "meaning": "内部表現をもとに系列データを生成するネットワーク",
        "memo": "エンコーダで圧縮された情報を基に出力データを生成する。"
    },
    "エンコーダ": {
        "meaning": "系列データを内部表現に変換するネットワーク",
        "memo": "入力された系列データを圧縮して内部表現に変換する。"
    },
    "sequence-to-sequence": {
        "meaning": "系列データから系列データを生成するモデル",
        "memo": "翻訳や要約生成など、入力と出力が異なる長さのタスクに使用される。"
    },
    "Bidirectional RNN": {
        "meaning": "順方向と逆方向に伝播するRNN",
        "memo": "前後のコンテキスト情報を考慮して学習を行う。"
    },
    "更新ゲート": {
        "meaning": "GRUにおいて新しい情報の追加を制御する役割",
        "memo": "新しいデータの影響度を調整し、長期的な依存関係を学習する。"
    },
    "リセットゲート": {
        "meaning": "GRUにおいて過去の記憶をリセットする役割",
        "memo": "無関係な情報をリセットし、効率的な学習を実現する。"
    },
    "GRU": {
        "meaning": "LSTMの簡略版で2つのゲートのみを持つ構造",
        "memo": "パラメータが少なく、計算効率が高い。"
    },
    "忘却ゲート": {
        "meaning": "セルの状態をどれだけ保持するかを制御するゲート",
        "memo": "不要な情報を削除し、学習効率を改善する役割を持つ。"
    },
    "出力ゲート": {
        "meaning": "内部状態から出力を生成するゲート",
        "memo": "LSTMの内部記憶を次の隠れ層へ出力する役割を担う。"
    },
    "入力ゲート": {
        "meaning": "新たな情報をセル状態に追加する役割を持つゲート",
        "memo": "入力データのうち、どれだけ内部状態に追加するかを制御する。"
    },
    "CEC(Constant Error Carousel)": {
        "meaning": "誤差を内部に留めて伝播させるメカニズム",
        "memo": "誤差が減衰せず、長期依存の学習が可能となる。"
    },
    "LSTMブロック": {
        "meaning": "LSTMの基本構造単位",
        "memo": "複数のゲート(入力、出力、忘却)と記憶セル(CEC)から構成される。"
    },
    "LSTM": {
        "meaning": "長期依存関係を学習可能なRNNの拡張版",
        "memo": "勾配消失問題を解決するために開発され、入力ゲート、出力ゲート、忘却ゲートを持つ。"
    },
    "出力重み衝突": {
        "meaning": "複数の出力が重みを通じて干渉し合い、誤差が増大する現象。",
        "memo": "次のステップへの影響が不安定になることで、学習が遅延する。"
    },
    "入力重み衝突": {
        "meaning": "複数の入力が同時に影響し合い、学習が不安定になる現象",
        "memo": "時系列データにおいて、関連の強い入力同士が干渉する。"
    },
    "勾配消失問題": {
        "meaning": "勾配が小さくなりすぎて学習が進まない問題",
        "memo": "シグモイド関数やTanh関数で発生しやすい。深いネットワークで多発。"
    },
    "ジョルダンネットワーク": {
        "meaning": "RNNの一種で出力を次のステップに戻す構造",
        "memo": "出力層の結果を次の入力へフィードバックする構造。短期の時系列データに強い。"
    },
    "エルマンネットワーク": {
        "meaning": "単純なRNNの一種",
        "memo": "隠れ層の出力を次の入力へフィードバックする構造。短期の時系列データに強い。"
    },
    " Pretrained Models": {
        "meaning": "事前に大規模データで学習されたモデル",
        "memo": "BERT、GPT、RoBERTaなど、転移学習で利用される。大規模データでの学習済みで高精度な結果を出力する。"
    },
    "言語モデル": {
        "meaning": "文章の出現確率を予測するモデル",
        "memo": "次に来る単語や文章全体の確率を計算し、文章生成や文脈理解に利用される。"
    },
    "リカレントニューラルネットワーク(RNN)": {
        "meaning": "自系列データや系列データを扱うためのニューラルネットワーク",
        "memo": "前の時間ステップの情報を保持し、次のステップに引き継ぐことで、連続したデータの学習が可能。"
    },
    "グループ正規化": {
        "meaning": "複数のチャンネルをグループに分けて正規化",
        "memo": "バッチサイズに依存せず安定した学習を実現。小規模バッチでも有効。"
    },
    "インスタンス正規化": {
        "meaning": "各サンプルごとに正規化を行う手法",
        "memo": "画像生成モデルなどで使用され、スタイル変換で効果的。"
    },
    "レイヤー正規化": {
        "meaning": "各レイヤー内のニューロン単位で正則化する手法",
        "memo": "RNNやTransformerなど、バッチサイズに依存しないモデルで有効。"
    },
    "バッチ正則化": {
        "meaning": "バッチごとに平均と分散を用いて正規化する手法。",
        "memo": "学習を高速化し、初期値や学習率への依存を減らす。"
    },
    "正規化処理": {
        "meaning": "データ分布を調整し学習を安定化する処理",
        "memo": "内部共変量シフトを抑制し、学習効率を改善する。"
    },
    "ResNet": {
        "meaning": "スキップ結合を用いたディープニューラルネットワーク",
        "memo": "非常に深いネットワークでも学習が可能。スキップ結合により勾配消失問題を解決。"
    },
    "スキップ結合": {
        "meaning": "層をまたいで出力する手法",
        "memo": "学習の停滞を紡ぎ、勾配消失問題を緩和する。ResNetに代表される。"
    },
    "Global Average Pooling": {
        "meaning": "特徴マップ全体の平均値を出力する手法",
        "memo": "全結合層の代替として用いられ、パラメータ数を削減する。"
    },
    "全結合層": {
        "meaning": "すべてのニューロンが接続される層",
        "memo": "畳み込み層の後段に用いられ、最終的な判断を行う。"
    },
    "不変性": {
        "meaning": "変化しても出力が変わらない性質",
        "memo": "位置、回転、スケールなどに対する安定性。"
    },
    "平均値プーリング": {
        "meaning": "領域内の平均値を出力するプーリング手法",
        "memo": "特徴のなだらかな要約に使われる。"
    },
    "最大値プーリング": {
        "meaning": "領域内の最大値を出力するプーリング手法",
        "memo": "特徴量の抽出に使われ、ノイズ耐性をもつ。"
    },
    "ダウンサンプリング(サブサンプリング)": {
        "meaning": "データを間引いてサイズを縮小する処理",
        "memo": "プーリング層やストライドで実現される。"
    },
    "プーリング層": {
        "meaning": "特徴マップのダウンサンプリングを行う",
        "memo": "特徴量の空間サイズを縮小し、計算負荷と過学習を抑制する。"
    },
    "Pointwise Convolution": {
        "meaning": "1×1のカーネルを用いた畳み込み処理",
        "memo": "チャンネル間の結合を行う。Depthwise Convolutionの後に適用される。"
    },
    "Depthwise Convolution": {
        "meaning": "チャンネル事に個別に畳み込み処理",
        "memo": "各チャンネルに独立したカーネルを適用する。"
    },
    "Depthwise Separable Convolution": {
        "meaning": "空間方向とチャンネル方向を分けて畳み込む手法",
        "memo": "Depthwise ConvolutionとPointwise Convolutionの組み合わせで、計算効率を高める。"
    },
    "Atrous Convolution(Dilated Convolution)": {
        "meaning": "間隔を空けたカーネル適用による畳み込み処理",
        "memo": "有効受容野を広げつつ、計算量を増やさずに特徴を捉える手法。"
    },
    "ストライド": {
        "meaning": "カーネルを適用する際の移動幅",
        "memo": "大きいほど出力が縮小され、計算量が減少する。"
    },
    "バディング処理": {
        "meaning": "入力データの周囲に0などを加える処理",
        "memo": "出力サイズを調整するために行う。ゼロパティングが一般的。"
    },
    "カーネル(フィルタ)": {
        "meaning": "特徴抽出のために用いる重み行列",
        "memo": "畳み込み処理で用いられる小さな重み行列。特徴検出の役割を持つ。"
    },
    "畳み込み処理": {
        "meaning": "カーネルを使って特徴を抽出する演算",
        "memo": "カーネルをデータにスライド適用し、内積を計算して特徴マップを生成する。"
    },
    "畳み込み層": {
        "meaning": "カーネルを用いて特徴を抽出する層",
        "memo": "画像や信号データから局所的特徴を抽出する。CNNの基本的な構成要素。"
    },
    "Randomized ReLU": {
        "meaning": "LeakyReLUの勾配をランダムに決める活性化関数",
        "memo": "学習時に負の領域の傾きをランダムに決め、汎化性能向上を狙う。"
    },
    "Parametric ReLU": {
        "meaning": "LeakyReLUの勾配を学習可能にした活性化関数",
        "memo": "負の領域の勾配をパラメータとして学習できるようにした改良版。"
    },
    "Leaky ReLU関数": {
        "meaning": "ReLUの変種で、０以下もわずかに勾配を持つ関数",
        "memo": "勾配が0になる問題を回避するため、負の領域でも微小な傾きを持つ。"
    },
    "ReLU関数": {
        "meaning": "0以下は0、0より大きければそのまま出力する活性化関数",
        "memo": "勾配消失問題を緩和する効果があり、DNNで広く使用される。"
    },
    "Tnsh関数": {
        "meaning": "出力が-1から1に収まるS字型活性化関数",
        "memo": "Sigmoid関数と似るが、出力の中心が0であり、学習が安定しやすい。"
    },
    "導関数": {
        "meaning": "関数の微分係数",
        "memo": "関数の変化率を表す。勾配降下法や最適化において基本となる概念。"
    },
    "勾配爆発問題": {
        "meaning": "勾配が大きくなりすぎて学習が不安定になる問題",
        "memo": "RNNや深いネットワークで起こりやすい。勾配クリッピングで対処されることが多い。"
    },
    "信用割当問題": {
        "meaning": "どの行動が報酬に貢献したかを特定する問題",
        "memo": "強化学習などで、結果に対してどの要素がどのくらい貢献したかを適切に割り当てる必要がある問題。"
    },
    "連鎖率": {
        "meaning": "合成関数の微分を行う公式",
        "memo": "各関数の微分を順番に掛け合わせることで、全体の微分が求まる。"
    },
    "合成関数の微分": {
        "meaning": "複数の関数を合成した場合の微分",
        "memo": "連鎖律(チェーンルール)を用いて複数の関数を順に微分する。誤差逆伝播法の基盤。"
    },
    "遺伝子的アルゴリズム": {
        "meaning": "生物の進化過程を模倣した最適化アルゴリズム",
        "memo": "交叉、突然変異、選択を繰り返しながら最適解を探索する進化的手法。"
    },
    "ベイズ最適化": {
        "meaning": "確率モデルを用いてハイパーパラメータ探索を効率化する手法",
        "memo": "探索履歴を考慮しつつ、効果的に次の探索点を決める。"
    },
    "ランダムサーチ": {
        "meaning": "ランダムにハイパーパラメータを選んで探索する手法",
        "memo": "探索空間の無駄を省き、計算負担を軽減できる。"
    },
    "グリッドサーチ": {
        "meaning": "ハイパーパラメータの組み合わせを網羅的に探索する手法",
        "memo": "すべてのパラメータ候補を試すため計算コストが高いが、確実性がある。"
    },
    "ハイパーパラメータチューニング": {
        "meaning": "最適なハイパーパラメータを探索する作業",
        "memo": "学習率、バッチサイズ、層の数などの調整。性能向上のために重要。"
    },
    "二重降下現象": {
        "meaning": "モデルが大きくなるほど一旦過学習し、その後また汎化性能が上がる現象",
        "memo": "伝統的なU字型の汎化曲線を超え、過学習を一旦超えたあと性能が再び改善する。"
    },
    "ノーフリーランチ定理": {
        "meaning": "すべての問題に最適な学習アルゴリズムは存在しないという理論",
        "memo": "アルゴリズムの優劣は問題設定に依存する。どんな場合も勝てる方法はない。"
    },
    "早期終了": {
        "meaning": "検証データで性能が改善しなくなったら学習を停止する手法",
        "memo": "過学習を防ぐため、一定期間性能が向上しない場合に学習を打ち切る。"
    },
    "AMSBound": {
        "meaning": "AdaBoundの変種で安定性を向上した手法",
        "memo": "AdaBoundの改良版。学習率の変動をさらに制限して安定化。"
    },
    "AdaBound": {
        "meaning": "Adamに境界制約を加えた手法",
        "memo": "学習率が極端に大きくも小さくもならないように調整。"
    },
    "Adam": {
        "meaning": "モーメンタムとRMSpropを組み合わせた最適化手法",
        "memo": "最適化アルゴリズムのデファクトスタンダード。１次モーメンタムを利用。"
    },
    "RMSprop": {
        "meaning": "勾配の平方の移動平均で学習率を調整する手法",
        "memo": "Adagradの改良版で、学習率が過度に減衰するのを防ぐ。"
    },
    "Adadelta": {
        "meaning": "Adagradの過学習を抑える改良版",
        "memo": "累積平方勾配の減衰平均を使い、学習率が極端に小さくなる問題を解決。"
    },
    "Adagrad": {
        "meaning": "勾配の大きさに応じて学習率を自動調整する手法",
        "memo": "頻繁に更新されるパラメータの学習率を減少させる。スパースデータに強い。"
    },
    "モーメンタム": {
        "meaning": "過去の勾配情報を加味して更新する最適化手法",
        "memo": "慣性を利用して局所最適や鞍点を突破しやすくする手法。"
    },
    "プラトー": {
        "meaning": "損失関数の変化が非常に小さい平坦な領域",
        "memo": "学習が進みにくく停滞しやすいエリア。更新が難しい。"
    },
    "鞍点": {
        "meaning": "ある方向では最小、別の方向では最大となる点",
        "memo": "勾配がゼロになるが最適解ではない点。高次元空間で多く発生する。"
    },
    "大域最適解": {
        "meaning": "全ての候補の中で最も良い解",
        "memo": "最適化問題の解空間全体を考慮したときの真の最適解。"
    },
    "局所最適解": {
        "meaning": "その近傍では最も良いが、全体では最適でない解",
        "memo": "最適化において、より良い解が他に存在する可能性があるが、近くに改善点がない状態。"
    },
    "ミニバッチ学習": {
        "meaning": "ミニバッチ単位で学習を行う方式",
        "memo": "一般的に広く用いられる効率的な学習方式。"
    },
    "バッチサイズ": {
        "meaning": "一回の勾配計算に使うデータの数",
        "memo": "小さいとノイズが多く、大きいと計算コストが高い。適切なサイズ設定が必要。"
    },
    "ミニバッチ": {
        "meaning": "勾配計算に使う少数のデータセット",
        "memo": "ミニバッチサイズ(バッチサイズ)を適切に設定することで効率的な学習が可能。"
    },
    "ミニバッチ勾配降下法": {
        "meaning": "データを小さなグループ(ミニバッチ)に分けて学習する手法",
        "memo": "SGDとバッチ学習の中間的手法。安定性と効率性を兼ね備える。"
    },
    "エポック": {
        "meaning": "全データセットを１回学習する単位",
        "memo": "何回学習データを繰り返して使うかを表す指標。"
    }
}